ML-Based Python Code Summarization

Seq2Seq with Attention using PyTorch

ğŸ“Œ Project Overview

This project implements a machine learningâ€“based system for automatic Python code summarization.
It generates natural language descriptions for Python functions using a Sequence-to-Sequence (Seq2Seq) neural network with an attention mechanism, built using PyTorch.

Automated code summarization helps developers understand, document, and maintain large codebases efficiently.

ğŸ¯ Motivation

Understanding source code manually is time-consuming, especially in large projects.
This project aims to:

Improve code readability

Assist in automatic documentation

Demonstrate practical NLP techniques applied to source code

âœ¨ Features

Python code â†’ English summary generation

Seq2Seq encoderâ€“decoder architecture

Attention mechanism for better context understanding

Complete preprocessing pipeline

Training and inference scripts

Reproducible dataset download process

ğŸ§  Model Architecture

Encoder: LSTM-based encoder for tokenized Python code

Decoder: LSTM-based decoder with attention

Attention: Bahdanau-style attention

Loss Function: Cross-Entropy Loss

Optimizer: Adam

Framework: PyTorch

ğŸ“Š Dataset Information

Dataset Type: Python code and corresponding natural language summaries

Source: Hugging Face (e.g., CodeSearchNet-style dataset)

Format: JSON

Note:
The dataset is not included in this repository due to GitHubâ€™s file size limitations (>100MB).
A script is provided to download and prepare the dataset automatically.

ğŸ—‚ Project Structure
ML-Based-Python-Code-Summarization/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dataset.py          # Dataset loading logic
â”‚   â”œâ”€â”€ preprocessing.py   # Tokenization and preprocessing
â”‚   â”œâ”€â”€ vocab.py            # Vocabulary handling
â”‚   â”œâ”€â”€ train.py            # Model training script
â”‚   â”œâ”€â”€ infer.py            # Inference / prediction script
â”‚   â”‚
â”‚   â””â”€â”€ model/
â”‚       â”œâ”€â”€ encoder.py      # Encoder implementation
â”‚       â”œâ”€â”€ decoder.py      # Decoder implementation
â”‚       â”œâ”€â”€ attention.py    # Attention mechanism
â”‚       â””â”€â”€ seq2seq.py      # Seq2Seq wrapper
â”‚
â”œâ”€â”€ download_data_hf.py     # Dataset download script
â”œâ”€â”€ README.md               # Project documentation
â”œâ”€â”€ .gitignore              # Ignored files (datasets, venv, models)
â””â”€â”€ requirements.txt        # Python dependencies

ğŸ’» Requirements

Python 3.8+

Git

Internet connection (for dataset download)

ğŸ”§ Installation & Setup
1ï¸âƒ£ Clone the Repository
git clone https://github.com/NirmaAbro/ML-Based-Python-Code-Summarization.git
cd ML-Based-Python-Code-Summarization

2ï¸âƒ£ Create and Activate Virtual Environment
Windows
python -m venv venv
source venv/Scripts/activate

Linux / macOS
python3 -m venv venv
source venv/bin/activate

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

ğŸ“¥ Dataset Download

Run the following command to download and prepare the dataset:

python download_data_hf.py


ğŸ“Œ The dataset will be stored locally inside the data/ directory.

ğŸ‹ï¸ Training the Model

To train the model, run:

python src/train.py


Training progress will be displayed in the terminal

The trained model will be saved locally

Training time depends on dataset size and hardware

ğŸ” Running Inference (Generate Summary)

After training, generate summaries using:

python src/infer.py


You can modify infer.py to input your own Python code snippets.

ğŸ“ˆ Evaluation

The model is evaluated using BLEU score

Qualitative evaluation is also performed by comparing generated summaries with ground-truth descriptions

ğŸ§ª Example Result

Input Code:

def multiply(a, b):
    return a * b


Generated Summary:

Returns the product of two numbers

âš ï¸ Limitations

Performance depends on dataset quality

Long and complex code snippets may reduce accuracy

Vocabulary size is limited

Seq2Seq models struggle with very large contexts

ğŸš€ Future Improvements

Replace Seq2Seq with Transformer-based models (CodeBERT, T5)

Support multiple programming languages

Improve evaluation metrics

Add web-based UI for live inference

ğŸ›  Technologies Used

Python

PyTorch

Hugging Face Datasets

NumPy

Git & GitHub

ğŸ‘¤ Author

Nirma Abro
Machine Learning Project
Academic / Research-Oriented Implementation

ğŸ“„ License

This project is intended for educational and research purposes.
